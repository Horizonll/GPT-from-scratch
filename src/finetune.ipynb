{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11047dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stu12/miniconda3/envs/ml/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e721105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, context, model, tokenizer, max_length=256):\n",
    "    prompt = f\"Question: {question}\\nContext: {context}\\nAnswer:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=len(inputs.input_ids[0]) + max_length,\n",
    "            temperature=0.7,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    answer_start = answer.find(\"Answer:\") + len(\"Answer:\")\n",
    "    return answer[answer_start:].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a198e48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stu12/miniconda3/envs/ml/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The SARS-CoV-2 virus is a virus that is present in the human body. It is a virus that is present in the human body. It is a virus that is present in the human body. It is a virus that is present in the human body. It is a virus that is present in the human body. It is a virus that is present in the human body. It is a virus that is present in the human body. It is a virus that is present in the human body. It is a virus that is present in the human body. It is a virus that is present in the human body. It is a virus that is present in the human body. It is a virus that is present in the human body. It is a virus that is present in the human body. It is a virus that is present in the human body. It is a virus that is present in the human body. It is a virus that is present in the human body. It is a virus that is present in the human body. It is a virus that is present in the human body. It is a virus that is present in the human body. It is a virus that is present in the human body. It is a virus that is present in\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"pubmed_qa\", \"pqa_labeled\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "question = \"What causes COVID-19?\"\n",
    "context = \"COVID-19 is caused by the SARS-CoV-2 virus.\"\n",
    "answer = generate_answer(question, context, model, tokenizer)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ee15b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:02<00:00, 409.66 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    texts = []\n",
    "    for question, context, answer in zip(\n",
    "        examples[\"question\"], examples[\"context\"], examples[\"final_decision\"]\n",
    "    ):\n",
    "        context_text = \" \".join(context) if isinstance(context, list) else context\n",
    "        text = f\"Question: {question}\\nContext: {context_text}\\nAnswer: {answer}\"\n",
    "        texts.append(text)\n",
    "    tokenized = tokenizer(texts, truncation=True, padding=\"max_length\", max_length=512)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "if \"validation\" not in tokenized_dataset:\n",
    "    split_dataset = tokenized_dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "    train_dataset = split_dataset[\"train\"]\n",
    "    eval_dataset = split_dataset[\"test\"]\n",
    "else:\n",
    "    train_dataset = tokenized_dataset[\"train\"]\n",
    "    eval_dataset = tokenized_dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf16b02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity before finetune: 28.32\n"
     ]
    }
   ],
   "source": [
    "def calculate_perplexity(model, eval_dataset, batch_size=4):\n",
    "    model.eval()\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "    data_loader = DataLoader(\n",
    "        eval_dataset, batch_size=batch_size, shuffle=False, collate_fn=data_collator\n",
    "    )\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            outputs = model(**inputs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item() * labels.size(0) * labels.size(1)\n",
    "            total_tokens += torch.sum(labels != -100).item()\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    return perplexity.item()\n",
    "\n",
    "\n",
    "eval_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "pre_finetune_perplexity = calculate_perplexity(model, eval_dataset)\n",
    "print(f\"Perplexity before finetune: {pre_finetune_perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e08b86e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='675' max='675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [675/675 01:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.255400</td>\n",
       "      <td>2.195654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity after finetune: 8.78\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=3,\n",
    "    warmup_steps=500,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model(\"../results/fine_tuned_gpt2_pubmedqa\")\n",
    "tokenizer.save_pretrained(\"../results/fine_tuned_gpt2_pubmedqa\")\n",
    "results = trainer.evaluate()\n",
    "print(f\"Perplexity after finetune: {torch.exp(torch.tensor(results['eval_loss'])):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "672882c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stu12/miniconda3/envs/ml/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The aim of this study was to determine the cause of COVID-19 in a population of patients with severe respiratory syndrome (SARS).\n",
      "METHODS: A retrospective cohort study was conducted in the Netherlands. Patients with severe SARS were included in the study. Patients with severe SARS were excluded from the study because of the presence of SARS-CoV-2 virus.\n",
      "RESULTS: The mean age of the patients was 43.5 years (range, 30-54 years). The mean age of the patients was 65.4 years (range, 45-69 years). The mean age of the patients was 65.4 years (range, 45-69 years). The mean age of the patients was 65.4 years (range, 45-69 years). The mean age of the patients was 65.4 years (range, 45-69 years). The mean age of the patients was 65.4 years (range, 45-69 years). The mean age of the patients was 65.4 years (range, 45-69 years). The mean age of the patients was 65.4 years (range, 45-69 years). The mean age of the patients was 65.4 years (range, 45-69 years). The mean age of the\n"
     ]
    }
   ],
   "source": [
    "model_finetune = GPT2LMHeadModel.from_pretrained(\n",
    "    \"../results/fine_tuned_gpt2_pubmedqa\"\n",
    ").to(device)\n",
    "tokenizer_finetune = GPT2Tokenizer.from_pretrained(\n",
    "    \"../results/fine_tuned_gpt2_pubmedqa\"\n",
    ")\n",
    "\n",
    "answer = generate_answer(question, context, model_finetune, tokenizer_finetune)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
